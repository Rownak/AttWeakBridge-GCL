{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12829ceb-bfe8-45d3-8fa6-ff2dc89da108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5099e0-f3e1-4c68-8830-399d1849b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, text_embedding_dim, graph_embedding_dim, hidden_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(text_embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, graph_embedding_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, text_embedding):\n",
    "        graph_embedding = self.model(text_embedding)\n",
    "        return graph_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01aa0e89-20a7-4503-8e87-6a5ad2253359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, text_embedding_dim, graph_embedding_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(text_embedding_dim + graph_embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # Add dropout to prevent discriminator from becoming too confident\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, text_embedding, graph_embedding):\n",
    "        combined = torch.cat((text_embedding, graph_embedding), dim=1)\n",
    "        validity = self.model(combined)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46966a33-565f-4cbb-9288-1753ad6de1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, text_embeddings, graph_embeddings):\n",
    "        self.text_embeddings = torch.tensor(text_embeddings, dtype=torch.float32)\n",
    "        self.graph_embeddings = torch.tensor(graph_embeddings, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_embedding = self.text_embeddings[idx]\n",
    "        graph_embedding = self.graph_embeddings[idx]\n",
    "        return text_embedding, graph_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4976d5-3a0c-45ec-8869-3b665a7576f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "total_obj=1136\n",
    "paragraphs = []\n",
    "with open('../../ics_cwe/id_to_desc.json') as fp:\n",
    "    id_to_desc = json.load(fp)\n",
    "for i in range(total_obj):\n",
    "    paragraphs.append(id_to_desc[str(i)])\n",
    "graph_embeddings = np.load(\"../../ics_cwe/{}/sample_{}/{}/text_hop_dual_gm_1.0.npy\".format(\"GCN\",4,\"pt_Gpt2\"))\n",
    "text_embeddings = np.load(\"../../ics_cwe/Text_Hop/{}/data/all_embeddings.npy\".format(\"pt_Gpt2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8720e9e5-ed2e-41d9-a834-38b1e4a1d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "text_embedding_dim = text_embeddings.shape[1]  # Example text embedding dimension\n",
    "graph_embedding_dim = graph_embeddings.shape[1]  # Desired graph embedding dimension\n",
    "hidden_dim = 128\n",
    "lr = 0.0002\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "epochs = 10000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3a341f-d940-4fb7-966e-75187189ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "embedding_dataset = EmbeddingDataset(text_embeddings, graph_embeddings)\n",
    "# Create the DataLoader\n",
    "embedding_dataloader = DataLoader(embedding_dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff00ed-5179-498f-b6a6-3905b5aa8e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4f035-4753-4ec9-9f7c-cffc45e5a6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51def62-224e-4b7c-b826-f5a3ddbbf5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the networks and move to device\n",
    "generator = Generator(text_embedding_dim, graph_embedding_dim, hidden_dim).to(device)\n",
    "discriminator = Discriminator(text_embedding_dim, graph_embedding_dim, hidden_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13e3a09-ef92-45e3-923e-77bac8f72b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss and optimizers\n",
    "# adversarial_loss = nn.BCELoss().to(device)\n",
    "# optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# Loss and optimizers\n",
    "adversarial_loss = nn.BCELoss().to(device)\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726de162-500f-489a-9e3d-19e48d6d1636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000]  D Loss: 0.6525412201881409  G Loss: 0.8440119028091431\n",
      "Epoch [100/10000]  D Loss: 0.5759603977203369  G Loss: 1.0861787796020508\n",
      "Epoch [200/10000]  D Loss: 0.4503016769886017  G Loss: 1.7135488986968994\n",
      "Epoch [300/10000]  D Loss: 0.38879868388175964  G Loss: 2.5334699153900146\n",
      "Epoch [400/10000]  D Loss: 0.32022085785865784  G Loss: 4.027320861816406\n",
      "Epoch [500/10000]  D Loss: 0.3064548671245575  G Loss: 4.074382781982422\n",
      "Epoch [600/10000]  D Loss: 0.3373211622238159  G Loss: 5.704192161560059\n"
     ]
    }
   ],
   "source": [
    "# Training the cGAN\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (real_text_embeddings, real_graph_embeddings) in enumerate(embedding_dataloader):\n",
    "        batch_size = real_text_embeddings.size(0)\n",
    "\n",
    "        # Move data to device\n",
    "        real_text_embeddings = real_text_embeddings.to(device)\n",
    "        real_graph_embeddings = real_graph_embeddings.to(device)\n",
    "\n",
    "        # Adversarial ground truths with one-sided label smoothing\n",
    "        valid = torch.full((batch_size, 1), 0.9, device=device, requires_grad=False)  # Valid labels as 0.9\n",
    "        fake = torch.zeros((batch_size, 1), device=device, requires_grad=False)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate graph embeddings\n",
    "        generated_graph_embeddings = generator(real_text_embeddings)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(real_text_embeddings, generated_graph_embeddings), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real graph embeddings\n",
    "        real_loss = adversarial_loss(discriminator(real_text_embeddings, real_graph_embeddings), valid)\n",
    "        # Loss for fake graph embeddings\n",
    "        fake_loss = adversarial_loss(discriminator(real_text_embeddings, generated_graph_embeddings.detach()), fake)\n",
    "        # Total discriminator loss\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "    if(epoch%100==0):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]  D Loss: {d_loss.item()}  G Loss: {g_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00864385-edcf-437d-9429-b9d27cee1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the model\n",
    "model_save_path = 'generator_model.pth'\n",
    "\n",
    "# Save the state dictionary of the generator\n",
    "torch.save(generator.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Generator model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c1b13-f0dd-47d4-ab0f-5bd1f2e2ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator.load_state_dict(torch.load('generator_model.pth'))  # Load the trained model weights\n",
    "generator.eval()  # Set the generator to evaluation mode\n",
    "\n",
    "# Load new text embeddings\n",
    "new_text_embeddings = text_embeddings\n",
    "new_text_embeddings = torch.tensor(new_text_embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "# Generate graph embeddings for the new text embeddings\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    generated_graph_embeddings = generator(new_text_embeddings)\n",
    "\n",
    "# Convert the generated embeddings back to numpy if needed\n",
    "generated_graph_embeddings = generated_graph_embeddings.cpu().numpy()\n",
    "\n",
    "print(\"Generated Graph Embeddings:\", generated_graph_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2f50da-597a-4d8d-abe0-4846239c7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../ics_cwe/Text_Hop/{}/data/GAN_generated.npy\".format(\"pt_Gpt2\"), 'wb') as f:\n",
    "    np.save(f,generated_graph_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9bdc5-464e-4ac2-a325-58bf4218c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_graph_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572b218-6cc7-474f-bcd8-9459ba05fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220d068-f9c1-4ab1-9eef-e649b669ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(generated_graph_embeddings, graph_embeddings)\n",
    "sim_vec = []\n",
    "for i in range(len(graph_embeddings)):\n",
    "    sim_vec.append(cosine_sim_matrix[i][i])\n",
    "# print(cosine_sim_matrix.shape)  # Should print (203, 933)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e79d8-60d4-47b4-8119-e8db7a68eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e00bb3-43a1-4829-bc0e-e6056c26c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.hist(sim_vec, bins=10, edgecolor='black')  \n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Histogram of GAN Gen. Emb VS Graph Emb')\n",
    "plt.xlabel('Level')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig(\"GAN_sim_vec.png\", dpi=300, bbox_inches='tight')\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1373eab-b6da-43a4-b1e7-7334e72e6b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea5f43-3926-43aa-81b7-e11760dc5d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
