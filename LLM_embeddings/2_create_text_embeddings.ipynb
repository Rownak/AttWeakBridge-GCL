{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a590aa-217b-4fe0-813a-dda2aea99671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca6aa27-c76d-4fd7-8536-74dbaf6b773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"ics_attack\"\n",
    "EMBEDDING_SIZE = 1600\n",
    "data_dir = \"../datasets/\"+dataset_name+\"/\"\n",
    "model_output_dir = \"../model_outputs/\"+dataset_name+\"/llm_finetuned_models/\"\n",
    "model_name = \"gpt2-xl\"\n",
    "model_path = model_output_dir+model_name+\"/epoch_10/\"\n",
    "embeddings_path = \"../model_outputs/embeddings/\"+model_name+\"/epoch_10/\"\n",
    "# model_name = \"pt_gpt2-xl\"\n",
    "# model_path = model_output_dir+model_name+\"/\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if not os.path.exists(embeddings_path):\n",
    "    os.makedirs(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3d61831-2631-45f3-8825-708736f28ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class GPT2PromptTuningMixin:\n",
    "    def set_soft_prompt_embeds(self, soft_prompt_path):\n",
    "        self.soft_prompt = torch.load(soft_prompt_path)\n",
    "        self.n_tokens = self.soft_prompt.num_embeddings\n",
    "        print(f\"Set soft prompt! (n_tokens: {self.n_tokens})\")\n",
    "\n",
    "    def initialize_soft_prompt(self, n_tokens=20, initialize_from_vocab=True, random_range=0.5):\n",
    "        self.n_tokens = n_tokens\n",
    "        if initialize_from_vocab:\n",
    "            init_prompt_value = self.transformer.wte.weight[:n_tokens].clone().detach()\n",
    "        else:\n",
    "            init_prompt_value = torch.FloatTensor(n_tokens, self.config.n_embd).uniform_(-random_range, random_range)\n",
    "        self.soft_prompt = nn.Embedding(n_tokens, self.config.n_embd)\n",
    "        self.soft_prompt.weight = nn.parameter.Parameter(init_prompt_value)\n",
    "\n",
    "    def _cat_learned_embedding_to_input(self, input_ids):\n",
    "        inputs_embeds = self.transformer.wte(input_ids)\n",
    "        learned_embeds = self.soft_prompt.weight.repeat(inputs_embeds.size(0), 1, 1)\n",
    "        inputs_embeds = torch.cat([learned_embeds, inputs_embeds], dim=1)\n",
    "        return inputs_embeds\n",
    "\n",
    "    def _extend_labels(self, labels, ignore_index=-100):\n",
    "        n_batches = labels.shape[0]\n",
    "        return torch.cat(\n",
    "            [torch.full((n_batches, self.n_tokens), ignore_index).to(self.device), labels],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "    def _extend_attention_mask(self, attention_mask):\n",
    "        n_batches = attention_mask.shape[0]\n",
    "        return torch.cat(\n",
    "            [torch.full((n_batches, self.n_tokens), 1).to(self.device), attention_mask],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        if input_ids is not None:\n",
    "            inputs_embeds = self._cat_learned_embedding_to_input(input_ids).to(self.device)\n",
    "        if labels is not None:\n",
    "            labels = self._extend_labels(labels).to(self.device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = self._extend_attention_mask(attention_mask).to(self.device)\n",
    "\n",
    "        return super().forward(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=labels, **kwargs)\n",
    "class GPT2PromptTuningLM(GPT2PromptTuningMixin, GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1dcece9-d1b4-4484-87c7-ea87f3802191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path, soft_prompt_path, device='cuda'):\n",
    "    # Load the base GPT-2 model\n",
    "    print(\"loading soft prompt\")\n",
    "    base_model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n",
    "    \n",
    "    # Initialize the prompt tuning model\n",
    "    soft_model = GPT2PromptTuningLM.from_pretrained(\"gpt2-xl\", output_hidden_states=True, return_dict=True)\n",
    "    soft_model.set_soft_prompt_embeds(soft_prompt_path)\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "    # Move model to the specified device\n",
    "    soft_model.to(device)\n",
    "    \n",
    "    return soft_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335c66ae-8343-4cb7-acca-692c2c4a4acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, AutoTokenizer, AutoModelForMaskedLM, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from peft import PeftModel, PeftConfig\n",
    "from GPTPromptTuningModel import GPT2PromptTuningLM\n",
    "\n",
    "def getModel(isPretrained, model_path):\n",
    "\n",
    "    if(not isPretrained):\n",
    "        if(model_name==\"soft\"):\n",
    "            # Define paths\n",
    "            model_path = \"./trained_models/SOFT_GPT2-XL_E2\"\n",
    "            soft_prompt_path = \"./trained_models/SOFT_GPT2-XL_E2/soft_prompt.model\"\n",
    "            # Load the model and tokenizer\n",
    "            model, tokenizer = load_model_and_tokenizer(model_path, soft_prompt_path)\n",
    "            return model, tokenizer\n",
    "            \n",
    "        if(model_name==\"lora\"):\n",
    "            print(\"Load finetuned model:\")\n",
    "            # Path to your fine-tuned model\n",
    "            model_path = MODEL_PATHS[model_name]\n",
    "            base_model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n",
    "            base_model.to(device)\n",
    "            # Load the fine-tuned model\n",
    "            lora_model = PeftModel.from_pretrained(base_model,model_path,is_trainable=False)\n",
    "            lora_model.to(device)\n",
    "            # Load the tokenizer used during fine-tuning\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "            return lora_model, tokenizer\n",
    "        if(model_name==\"gpt2-xl\"):\n",
    "            print(\"Load finetuned model:\")\n",
    "            # Load the fine-tuned model\n",
    "            model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "            model.to(device)\n",
    "            # Load the tokenizer used during fine-tuning\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "            # Set padding token\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            return model, tokenizer\n",
    "        elif(model_name==\"SecureBert_E5\"):\n",
    "            print(\"Load finetuned model:\")\n",
    "            # Path to your fine-tuned model\n",
    "            model_path = MODEL_PATHS[model_name]\n",
    "            \n",
    "            # Load the fine-tuned model\n",
    "            model = RobertaForMaskedLM.from_pretrained(model_path)\n",
    "            model.to(device)\n",
    "            # Load the tokenizer used during fine-tuning\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "            # Set padding token\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            return model, tokenizer\n",
    "        elif(model_name==\"SecBert_E5\"):\n",
    "            print(\"Load finetuned model:\")\n",
    "            # Path to your fine-tuned model\n",
    "            model_path = MODEL_PATHS[model_name]\n",
    "            \n",
    "            # Load the fine-tuned model\n",
    "            model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "            model.to(device)\n",
    "            \n",
    "            # Load the tokenizer used during fine-tuning\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            # Set padding token\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            return model, tokenizer\n",
    "    else:\n",
    "        \n",
    "        if(model_name==\"pretrained_SecureBert\"):\n",
    "            print(\"Load pretrained_SecureBert model:\")\n",
    "            # Load pre-trained SecureBert tokenizer and model\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "            model = RobertaForMaskedLM.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "            model.to(device)\n",
    "        elif(model_name==\"pretrained_SecBert\"):\n",
    "            print(\"Load pretrained_SecBert model:\")\n",
    "            # Load pre-trained SecBert tokenizer and model\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"jackaduma/SecRoBERTa\")\n",
    "            \n",
    "            model = AutoModelForMaskedLM.from_pretrained(\"jackaduma/SecRoBERTa\")\n",
    "            model.to(device)\n",
    "        elif(model_name==\"pt_gpt2-xl\"):\n",
    "            print(\"Load pretrained gpt2 model:\")\n",
    "            # Load pre-trained Gpt2 tokenizer and model\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "            model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n",
    "            model.to(device)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7eff45b-018b-4a1f-8672-3982db9d953b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load finetuned model:\n"
     ]
    }
   ],
   "source": [
    "isPretrained = (model_name.split(\"_\")[0]==\"pt\")\n",
    "model, tokenizer = getModel(isPretrained, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10762ac-c235-491b-a6e1-7f885c3013c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode some input text\n",
    "# # Example1:  Monitor application logs for changes to settings and other events associated with network protocols that may be used to block communications.\n",
    "\n",
    "# # Example2: Adversaries may cause a sustained or permanent loss of view where the ICS equipment will require local, \n",
    "# # hands-on operator intervention; for instance, a restart or manual operation. \n",
    "# # By causing a sustained reporting or visibility loss, the adversary can effectively hide the present state of operations. \n",
    "# # This loss of view can occur without affecting the physical processes themselves.\n",
    "# # (Citation: Corero) (Citation: Michael J. Assante and Robert M. Lee) (Citation: Tyson Macaulay)\n",
    "\n",
    "# # Example3: A vulnerability classified as critical has been found in Tenda TX9 22.03.02.10. \n",
    "# # This affects the function sub_42CB94 of the file /goform/SetVirtualServerCfg. \n",
    "# # The manipulation of the argument list leads to stack-based buffer overflow. \n",
    "# # It is possible to initiate the attack remotely. The exploit has been disclosed to the public and may be used. \n",
    "# # The associated identifier of this vulnerability is VDB-261855. \n",
    "# # NOTE: The vendor was contacted early about this disclosure but did not respond in any way.\n",
    "\n",
    "# # Example4:A Path Traversal vulnerability in web component of Ivanti Avalanche before 6.4.3 allows \n",
    "# # a remote authenticated attacker to execute arbitrary commands as SYSTEM.\n",
    "# # Overview. A path traversal attack (also known as directory traversal) aims to access files \n",
    "# # and directories that are stored outside the web root folder.\n",
    "\n",
    "# # Example5: RCE vulnerabilities allow an attacker to execute arbitrary code on a remote device. \n",
    "# # An attacker can achieve RCE in a few different ways, including:\n",
    "# # Injection Attacks, Deserialization Attacks , Out-of-Bounds Write\n",
    "\n",
    "# input_ids = tokenizer.encode(\"A remote code execution vulnerability allow an attacker to\", return_tensors='pt').to(device)\n",
    "\n",
    "# # Generate text using the model\n",
    "# output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# # Decode the generated text\n",
    "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "910085c5-e89a-4c97-b3df-8929d711fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create embeddings. The methods will handle long texts. That is, \n",
    "## it will do chunking of tokens and then consolidate when input_text is long.\n",
    "\n",
    "\n",
    "def retrieveEmbeddings(input_text, tokenizer, model, device):\n",
    "    # Tokenize the input text to find out if splitting is needed\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_tensors = torch.tensor([input_ids]).to(device)\n",
    "    with torch.no_grad():\n",
    "        # outputs = model(input_tensors, output_hidden_states=True)\n",
    "        outputs = model.forward(input_tensors, output_hidden_states=True)\n",
    "    # Extract the hidden states\n",
    "    hidden_states = outputs.hidden_states\n",
    "    last_layer_embeddings = hidden_states[-1]\n",
    "    \n",
    "    # Mean pool the embeddings of the last layer across the sequence length dimension\n",
    "    mean_pooled = last_layer_embeddings.mean(dim=1)\n",
    "    \n",
    "    return mean_pooled.squeeze()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe5fa87-9362-4335-b8bd-ed7db3fb4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Adversaries may cause a sustained or permanent loss of view to operators and/or engineers. This may result in a loss of control, or a crash or other incident. (Citation: Department of Homeland Security September 2016) (Citation:\"\n",
    "embd = retrieveEmbeddings(text, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f471d27-dcd1-487e-be06-08ff849703b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24978f7",
   "metadata": {},
   "source": [
    "### Now I will create Embeddigns for the attack+weakness Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c026a9e6-840c-4827-bde5-32113fd19810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load JSON file\n",
    "import json\n",
    "with open(data_dir+'doc_id_to_desc.json') as f:\n",
    "    doc_id_to_desc = json.load(f)\n",
    "with open(data_dir+'doc_id_to_emb_id.json') as f:\n",
    "    doc_id_to_emb_id = json.load(f)\n",
    "with open(data_dir+'emb_id_to_doc_id.json') as f:\n",
    "    emb_id_to_doc_id = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "551ac693-fec6-4bcc-b2aa-64e59185c554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesed 0 th object: id: malware--a4a98eab-b691-45d9-8c48-869ef8fefd57\n",
      "Procesed 50 th object: id: course-of-action--1e7ccfc0-94c8-496e-8d27-032120892291\n",
      "Procesed 100 th object: id: malware--ac61f1f9-7bb1-465e-9b8a-c2ce8e88baf5\n",
      "Procesed 150 th object: id: attack-pattern--fab8fc7d-f27f-4fbb-9de6-44740aade05f\n",
      "Procesed 200 th object: id: x-mitre-data-component--74fa567d-bc90-425c-8a41-3c703abb221c\n",
      "Procesed 250 th object: id: 1053\n",
      "Procesed 300 th object: id: 595\n",
      "Procesed 350 th object: id: 118\n",
      "Procesed 400 th object: id: 1263\n",
      "Procesed 450 th object: id: 1277\n",
      "Procesed 500 th object: id: 1321\n",
      "Procesed 550 th object: id: 146\n",
      "Procesed 600 th object: id: 195\n",
      "Procesed 650 th object: id: 25\n",
      "Procesed 700 th object: id: 300\n",
      "Procesed 750 th object: id: 349\n",
      "Procesed 800 th object: id: 41\n",
      "Procesed 850 th object: id: 466\n",
      "Procesed 900 th object: id: 52\n",
      "Procesed 950 th object: id: 570\n",
      "Procesed 1000 th object: id: 914\n",
      "Procesed 1050 th object: id: 759\n",
      "Procesed 1100 th object: id: 831\n",
      "Processing of  1136 objects complete.\n",
      "1136 objects have valid text descriptions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "text_embeddings = [None for _ in range(len(emb_id_to_doc_id))]\n",
    "\n",
    "count=0\n",
    "i=0\n",
    "for doc_id in doc_id_to_desc:\n",
    "    text_data=doc_id_to_desc[doc_id]\n",
    "    embedding=[]\n",
    "    try:\n",
    "        embedding=retrieveEmbeddings(text_data, tokenizer, model, device)\n",
    "        count=count+1\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", e)        \n",
    "        print(\"i\", i)\n",
    "        print(\"Len:\", len(text_data))\n",
    "        print(text_data)\n",
    "        break\n",
    "    \n",
    "    text_embeddings[int(doc_id_to_emb_id[doc_id])]=embedding.detach().cpu().numpy()\n",
    "\n",
    "    if (i%50==0):\n",
    "        print(\"Procesed\", i,\"th object: id:\", doc_id)   \n",
    "    i+=1\n",
    "print(\"Processing of \", len(text_embeddings), \"objects complete.\")\n",
    "print(count, \"objects have valid text descriptions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df2ec1eb-dbf0-4123-af97-1314550a7386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "text_embeddings_np = np.array(text_embeddings)\n",
    "np.save(embeddings_path+\"text_embeddings.npy\",text_embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f93542-9f9e-431d-a78e-5a51e4441513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b0606e-c0a4-4d06-a1b1-82dbdb37aa1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e79330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
