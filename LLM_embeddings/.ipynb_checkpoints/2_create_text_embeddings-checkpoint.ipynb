{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a590aa-217b-4fe0-813a-dda2aea99671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, AutoTokenizer, AutoModelForMaskedLM, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import sys\n",
    "#base_dir = os.environ['AWEB_DIR']\n",
    "sys.path.append(\"../\")\n",
    "import config\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25945add-ee8e-4d35-afc1-c68952cdc537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca6aa27-c76d-4fd7-8536-74dbaf6b773f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/afarhan/post-doc/AWEB_GCL/datasets/enterprise_attack/\n",
      "/home/afarhan/post-doc/AWEB_GCL/model_outputs/enterprise_attack/llm_finetuned_models\n",
      "/home/afarhan/post-doc/AWEB_GCL/model_outputs/enterprise_attack/embeddings/pt_gpt2-xl\n"
     ]
    }
   ],
   "source": [
    "model_id=4\n",
    "dataset_name = config.ATTACK_DATASET\n",
    "data_dir = config.DATA_DIR\n",
    "#model_output_dir = os.path.join(base_dir, \"model_outputs\",dataset_name,\"llm_finetuned_models\" )\n",
    "model_output_dir = config.OUTPUT_DIR+\"llm_finetuned_models\"\n",
    "models = [\"pt_SecRoBERTa\", \"SecRoBERTa\", \"pt_SecureBERT\", \"SecureBERT\", \"pt_gpt2-xl\", \"gpt2-xl\"]\n",
    "model_name = models[model_id]\n",
    "no_epoch = config.LLM_FT_EPOCH\n",
    "isPretrained = (model_name.split(\"_\")[0] == \"pt\")\n",
    "model_path = os.path.join(model_output_dir, model_name, \"epoch_{}\".format(no_epoch))\n",
    "embeddings_path = config.EMBEDDING_DIR+model_name\n",
    "print(data_dir)\n",
    "print(model_output_dir)\n",
    "print(embeddings_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if not os.path.exists(embeddings_path):\n",
    "    os.makedirs(embeddings_path)\n",
    "\n",
    "with open(config.DESCRIPTION_FILE) as f:\n",
    "    doc_id_to_desc = json.load(f)\n",
    "with open(os.path.join(data_dir, 'doc_id_to_emb_id.json')) as f:\n",
    "    doc_id_to_emb_id = json.load(f)\n",
    "with open(os.path.join(data_dir, 'emb_id_to_doc_id.json')) as f:\n",
    "    emb_id_to_doc_id = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3d61831-2631-45f3-8825-708736f28ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class GPT2PromptTuningMixin:\n",
    "    def set_soft_prompt_embeds(self, soft_prompt_path):\n",
    "        self.soft_prompt = torch.load(soft_prompt_path)\n",
    "        self.n_tokens = self.soft_prompt.num_embeddings\n",
    "        print(f\"Set soft prompt! (n_tokens: {self.n_tokens})\")\n",
    "\n",
    "    def initialize_soft_prompt(self, n_tokens=20, initialize_from_vocab=True, random_range=0.5):\n",
    "        self.n_tokens = n_tokens\n",
    "        if initialize_from_vocab:\n",
    "            init_prompt_value = self.transformer.wte.weight[:n_tokens].clone().detach()\n",
    "        else:\n",
    "            init_prompt_value = torch.FloatTensor(n_tokens, self.config.n_embd).uniform_(-random_range, random_range)\n",
    "        self.soft_prompt = nn.Embedding(n_tokens, self.config.n_embd)\n",
    "        self.soft_prompt.weight = nn.parameter.Parameter(init_prompt_value)\n",
    "\n",
    "    def _cat_learned_embedding_to_input(self, input_ids):\n",
    "        inputs_embeds = self.transformer.wte(input_ids)\n",
    "        learned_embeds = self.soft_prompt.weight.repeat(inputs_embeds.size(0), 1, 1)\n",
    "        inputs_embeds = torch.cat([learned_embeds, inputs_embeds], dim=1)\n",
    "        return inputs_embeds\n",
    "\n",
    "    def _extend_labels(self, labels, ignore_index=-100):\n",
    "        n_batches = labels.shape[0]\n",
    "        return torch.cat(\n",
    "            [torch.full((n_batches, self.n_tokens), ignore_index).to(self.device), labels],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "    def _extend_attention_mask(self, attention_mask):\n",
    "        n_batches = attention_mask.shape[0]\n",
    "        return torch.cat(\n",
    "            [torch.full((n_batches, self.n_tokens), 1).to(self.device), attention_mask],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        if input_ids is not None:\n",
    "            inputs_embeds = self._cat_learned_embedding_to_input(input_ids).to(self.device)\n",
    "        if labels is not None:\n",
    "            labels = self._extend_labels(labels).to(self.device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = self._extend_attention_mask(attention_mask).to(self.device)\n",
    "\n",
    "        return super().forward(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=labels, **kwargs)\n",
    "class GPT2PromptTuningLM(GPT2PromptTuningMixin, GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1dcece9-d1b4-4484-87c7-ea87f3802191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path, soft_prompt_path, device='cuda'):\n",
    "    # Load the base GPT-2 model\n",
    "    print(\"loading soft prompt\")\n",
    "    base_model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n",
    "    \n",
    "    # Initialize the prompt tuning model\n",
    "    soft_model = GPT2PromptTuningLM.from_pretrained(\"gpt2-xl\", output_hidden_states=True, return_dict=True)\n",
    "    soft_model.set_soft_prompt_embeds(soft_prompt_path)\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "    # Move model to the specified device\n",
    "    soft_model.to(device)\n",
    "    \n",
    "    return soft_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335c66ae-8343-4cb7-acca-692c2c4a4acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# from transformers import RobertaTokenizer, RobertaForMaskedLM, AutoTokenizer, AutoModelForMaskedLM, GPT2Tokenizer, GPT2LMHeadModel\n",
    "# from peft import PeftModel, PeftConfig\n",
    "# from GPTPromptTuningModel import GPT2PromptTuningLM\n",
    "\n",
    "# def getModel(isPretrained, model_path):\n",
    "\n",
    "#     if(not isPretrained):\n",
    "#         if(model_name==\"gpt2-xl\"):\n",
    "#             print(\"Load finetuned model:\")\n",
    "#             # Load the fine-tuned model\n",
    "#             model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "#             model.to(device)\n",
    "#             # Load the tokenizer used during fine-tuning\n",
    "#             tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "#             # Set padding token\n",
    "#             if tokenizer.pad_token is None:\n",
    "#                 tokenizer.pad_token = tokenizer.eos_token\n",
    "#                 tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "#             return model, tokenizer\n",
    "#         elif(model_name==\"SecureBERT\"):\n",
    "#             print(\"Load finetuned SecureBERT model:\")\n",
    "#             # Load the fine-tuned model\n",
    "#             model = RobertaForMaskedLM.from_pretrained(model_path)\n",
    "#             model.to(device)\n",
    "#             # Load the tokenizer used during fine-tuning\n",
    "#             tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "#             # Set padding token\n",
    "#             if tokenizer.pad_token is None:\n",
    "#                 tokenizer.pad_token = tokenizer.eos_token\n",
    "#                 tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "#             return model, tokenizer\n",
    "#         elif(model_name==\"SecRoBERTa\"):\n",
    "#             print(\"Load finetuned SecRoBERTa model:\")\n",
    "#             # Load the fine-tuned model\n",
    "#             model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "#             model.to(device)\n",
    "            \n",
    "#             # Load the tokenizer used during fine-tuning\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#             # Set padding token\n",
    "#             if tokenizer.pad_token is None:\n",
    "#                 tokenizer.pad_token = tokenizer.eos_token\n",
    "#                 tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "#             return model, tokenizer\n",
    "#     else:\n",
    "        \n",
    "#         if(model_name==\"pt_SecureBERT\"):\n",
    "#             print(\"Load pretrained_SecureBert model:\")\n",
    "#             # Load pre-trained SecureBert tokenizer and model\n",
    "#             tokenizer = RobertaTokenizer.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "#             model = RobertaForMaskedLM.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "#             model.to(device)\n",
    "#         elif(model_name==\"pt_SecRoBERTa\"):\n",
    "#             print(\"Load pretrained_SecBert model:\")\n",
    "#             # Load pre-trained SecBert tokenizer and model\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(\"jackaduma/SecRoBERTa\")\n",
    "            \n",
    "#             model = AutoModelForMaskedLM.from_pretrained(\"jackaduma/SecRoBERTa\")\n",
    "#             model.to(device)\n",
    "#         elif(model_name==\"pt_gpt2-xl\"):\n",
    "#             print(\"Load pretrained gpt2 model:\")\n",
    "#             # Load pre-trained Gpt2 tokenizer and model\n",
    "#             tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "#             model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n",
    "#             model.to(device)\n",
    "#         if tokenizer.pad_token is None:\n",
    "#             tokenizer.pad_token = tokenizer.eos_token\n",
    "#             tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "#         return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53690cb-8a8a-4df0-8122-65239ee1f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(model_name, isPretrained, model_path):\n",
    "    if not isPretrained:\n",
    "        if model_name == \"gpt2-xl\":\n",
    "            print(\"Load finetuned GPT2 model:\")\n",
    "            model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "        elif model_name == \"SecureBERT\":\n",
    "            print(\"Load finetuned SecureBERT model:\")\n",
    "            model = RobertaForMaskedLM.from_pretrained(model_path)\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "        elif model_name == \"SecRoBERTa\":\n",
    "            print(\"Load finetuned SecRoBERTa model:\")\n",
    "            model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    else:\n",
    "        if model_name == \"pt_SecureBERT\":\n",
    "            print(\"Load pretrained_SecureBert model:\")\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "            model = RobertaForMaskedLM.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "        elif model_name == \"pt_SecRoBERTa\":\n",
    "            print(\"Load pretrained_SecBert model:\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"jackaduma/SecRoBERTa\")\n",
    "            model = AutoModelForMaskedLM.from_pretrained(\"jackaduma/SecRoBERTa\")\n",
    "        elif model_name == \"pt_gpt2-xl\":\n",
    "            print(\"Load pretrained gpt2 model:\")\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "            model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "    model.to(device)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7eff45b-018b-4a1f-8672-3982db9d953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model, tokenizer = getModel(isPretrained, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d10762ac-c235-491b-a6e1-7f885c3013c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode some input text\n",
    "# # Example1:  Monitor application logs for changes to settings and other events associated with network protocols that may be used to block communications.\n",
    "\n",
    "# # Example2: Adversaries may cause a sustained or permanent loss of view where the ICS equipment will require local, \n",
    "# # hands-on operator intervention; for instance, a restart or manual operation. \n",
    "# # By causing a sustained reporting or visibility loss, the adversary can effectively hide the present state of operations. \n",
    "# # This loss of view can occur without affecting the physical processes themselves.\n",
    "# # (Citation: Corero) (Citation: Michael J. Assante and Robert M. Lee) (Citation: Tyson Macaulay)\n",
    "\n",
    "# # Example3: A vulnerability classified as critical has been found in Tenda TX9 22.03.02.10. \n",
    "# # This affects the function sub_42CB94 of the file /goform/SetVirtualServerCfg. \n",
    "# # The manipulation of the argument list leads to stack-based buffer overflow. \n",
    "# # It is possible to initiate the attack remotely. The exploit has been disclosed to the public and may be used. \n",
    "# # The associated identifier of this vulnerability is VDB-261855. \n",
    "# # NOTE: The vendor was contacted early about this disclosure but did not respond in any way.\n",
    "\n",
    "# # Example4:A Path Traversal vulnerability in web component of Ivanti Avalanche before 6.4.3 allows \n",
    "# # a remote authenticated attacker to execute arbitrary commands as SYSTEM.\n",
    "# # Overview. A path traversal attack (also known as directory traversal) aims to access files \n",
    "# # and directories that are stored outside the web root folder.\n",
    "\n",
    "# # Example5: RCE vulnerabilities allow an attacker to execute arbitrary code on a remote device. \n",
    "# # An attacker can achieve RCE in a few different ways, including:\n",
    "# # Injection Attacks, Deserialization Attacks , Out-of-Bounds Write\n",
    "\n",
    "# input_ids = tokenizer.encode(\"A remote code execution vulnerability allow an attacker to\", return_tensors='pt').to(device)\n",
    "\n",
    "# # Generate text using the model\n",
    "# output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# # Decode the generated text\n",
    "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910085c5-e89a-4c97-b3df-8929d711fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create embeddings. The methods will handle long texts. That is, \n",
    "## it will do chunking of tokens and then consolidate when input_text is long.\n",
    "\n",
    "\n",
    "# def retrieveEmbeddings(input_text, tokenizer, model, device):\n",
    "#     # Tokenize the input text to find out if splitting is needed\n",
    "#     tokens = tokenizer.tokenize(input_text)\n",
    "#     input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "#     input_tensors = torch.tensor([input_ids]).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         # outputs = model(input_tensors, output_hidden_states=True)\n",
    "#         outputs = model.forward(input_tensors, output_hidden_states=True)\n",
    "#     # Extract the hidden states\n",
    "#     hidden_states = outputs.hidden_states\n",
    "#     last_layer_embeddings = hidden_states[-1]\n",
    "    \n",
    "#     # Mean pool the embeddings of the last layer across the sequence length dimension\n",
    "#     mean_pooled = last_layer_embeddings.mean(dim=1)\n",
    "    \n",
    "#     return mean_pooled.squeeze()\n",
    "\n",
    "# def retrieveEmbeddings(input_text, tokenizer, model, device):\n",
    "#     # Tokenize and encode the input text, ensuring proper padding and truncation\n",
    "#     encoded_input = tokenizer(\n",
    "#         input_text,\n",
    "#         max_length=512,  # Adjust this to match the model's max length\n",
    "#         padding='max_length',\n",
    "#         truncation=True,\n",
    "#         return_tensors='pt'\n",
    "#     ).to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**encoded_input, output_hidden_states=True)\n",
    "    \n",
    "#     # Extract the hidden states\n",
    "#     hidden_states = outputs.hidden_states\n",
    "#     last_layer_embeddings = hidden_states[-1]\n",
    "    \n",
    "#     # Mean pool the embeddings of the last layer across the sequence length dimension\n",
    "#     mean_pooled = last_layer_embeddings.mean(dim=1)\n",
    "    \n",
    "#     return mean_pooled.squeeze()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c231d13c-818d-496a-9241-e64418860e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveEmbeddings(input_text, tokenizer, model, device):\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    \n",
    "    # Initialize a list to store embeddings from each chunk\n",
    "    chunk_embeddings = []\n",
    "    \n",
    "    # Process text in chunks that fit within the model's limit\n",
    "    chunk_size = model.config.n_positions  # GPT-2's max sequence length\n",
    "    print(chunk_size)\n",
    "    print(len(tokens))\n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(chunk_tokens)\n",
    "        input_tensors = torch.tensor([input_ids]).to(device)\n",
    "        \n",
    "        # Forward pass, get hidden states for the chunk\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensors, output_hidden_states=True)\n",
    "        \n",
    "        # Extract the hidden states\n",
    "        hidden_states = outputs.hidden_states\n",
    "        last_layer_embeddings = hidden_states[-1]\n",
    "        \n",
    "        # Mean pool the embeddings of the last layer across the sequence length dimension\n",
    "        mean_pooled = last_layer_embeddings.mean(dim=1)\n",
    "        chunk_embeddings.append(mean_pooled)\n",
    "    # Concatenate embeddings from all chunks along the batch dimension\n",
    "    # and then take the mean across the concatenated dimension to get a single embedding\n",
    "    all_embeddings = torch.cat(chunk_embeddings, dim=0)\n",
    "    aggregated_embedding = all_embeddings.mean(dim=0)\n",
    "    \n",
    "    return aggregated_embedding.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6e9a6f4-4c90-4f6c-8b79-c680d15ee46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained gpt2 model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = getModel(model_name, isPretrained, model_path)\n",
    "text_embeddings = [None for _ in range(len(emb_id_to_doc_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32aa9633-3a8f-4163-a939-06fe50b70833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data=[]\n",
    "# for doc_id in doc_id_to_desc:\n",
    "#     text_data.append(doc_id_to_desc[doc_id])\n",
    "# max_length = [len(tokenizer.encode(text)) for text in text_data]  # Limit to 512 tokens\n",
    "# print(max_length.index(min(max_length)))\n",
    "# docid_list = list(doc_id_to_desc.keys())\n",
    "# print(docid_list[502])\n",
    "# txt1 = doc_id_to_desc['course-of-action--feff9142-e8c2-46f4-842b-bd6fb3d41157']\n",
    "# print(txt1)\n",
    "# len(tokenizer.encode(txt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fe5fa87-9362-4335-b8bd-ed7db3fb4c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#text = \"Adversaries may cause a sustained or permanent loss of view to operators and/or engineers. This may result in a loss of control, or a crash or other incident. (Citation: Department of Homeland Security September 2016) (Citation:\"\n",
    "embd = retrieveEmbeddings(txt1, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f471d27-dcd1-487e-be06-08ff849703b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24978f7",
   "metadata": {},
   "source": [
    "### Now I will create Embeddigns for the attack+weakness Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026a9e6-840c-4827-bde5-32113fd19810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load JSON file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ac693-fec6-4bcc-b2aa-64e59185c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text_embeddings = [None for _ in range(len(emb_id_to_doc_id))]\n",
    "\n",
    "count=0\n",
    "i=0\n",
    "for doc_id in doc_id_to_desc:\n",
    "    text_data=doc_id_to_desc[doc_id]\n",
    "    embedding=[]\n",
    "    try:\n",
    "        embedding=retrieveEmbeddings(text_data, tokenizer, model, device)\n",
    "        count=count+1\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", e)        \n",
    "        print(\"i\", i)\n",
    "        print(\"Len:\", len(text_data))\n",
    "        print(text_data)\n",
    "        break\n",
    "    \n",
    "    text_embeddings[int(doc_id_to_emb_id[doc_id])]=embedding.detach().cpu().numpy()\n",
    "\n",
    "    if (i%50==0):\n",
    "        print(\"Procesed\", i,\"th object: id:\", doc_id)   \n",
    "    i+=1\n",
    "print(\"Processing of \", len(text_embeddings), \"objects complete.\")\n",
    "print(count, \"objects have valid text descriptions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ec1eb-dbf0-4123-af97-1314550a7386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "text_embeddings_np = np.array(text_embeddings)\n",
    "np.save(embeddings_path+\"text_embeddings.npy\",text_embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f93542-9f9e-431d-a78e-5a51e4441513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b0606e-c0a4-4d06-a1b1-82dbdb37aa1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e79330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(isPretrained, model_path):\n",
    "\n",
    "    if(not isPretrained):\n",
    "        if(model_name==\"soft\"):\n",
    "            # Define paths\n",
    "            model_path = \"./trained_models/SOFT_GPT2-XL_E2\"\n",
    "            soft_prompt_path = \"./trained_models/SOFT_GPT2-XL_E2/soft_prompt.model\"\n",
    "            # Load the model and tokenizer\n",
    "            model, tokenizer = load_model_and_tokenizer(model_path, soft_prompt_path)\n",
    "            return model, tokenizer\n",
    "            \n",
    "        if(model_name==\"lora\"):\n",
    "            print(\"Load finetuned model:\")\n",
    "            # Path to your fine-tuned model\n",
    "            model_path = MODEL_PATHS[model_name]\n",
    "            base_model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n",
    "            base_model.to(device)\n",
    "            # Load the fine-tuned model\n",
    "            lora_model = PeftModel.from_pretrained(base_model,model_path,is_trainable=False)\n",
    "            lora_model.to(device)\n",
    "            # Load the tokenizer used during fine-tuning\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "            return lora_model, tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
