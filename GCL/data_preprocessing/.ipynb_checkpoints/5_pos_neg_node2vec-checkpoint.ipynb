{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5741b-be17-48ed-9714-69bb21b4987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "# Ensure to set the correct CUDA device if multiple GPUs are available\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9391b-8bfd-433a-a7cd-3d35d33aa9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"pretrained_SecBert\", \"SecBert_E5\", \"pretrained_SecureBert\",  \"SecureBert_E5\", \"pretrained_Gpt2\",\"Gpt2_E5\"]\n",
    "# models = [\"pretrained_SecBert\", \"SecBert\", \"pretrained_SecureBert\",  \"SecureBert\", \"pretrained_Gpt2\",\"Gpt2\"]\n",
    "#models = [\"pretrained_SecBert\",\"pretrained_SecureBert\",\"pretrained_Gpt2\"]\n",
    "current_model = models[5]\n",
    "sample = 4\n",
    "margin1=1.0\n",
    "margin2=1.0\n",
    "two_losses = True\n",
    "dir_name = \"../../ics_cwe/\"+current_model+\"/\"\n",
    "# Assuming predefined weights are stored in a numpy array named 'predefined_embeddings1_weights'\n",
    "text_embeddings = np.load(dir_name+'data/all_embeddings.npy')\n",
    "with open('../../ics_cwe/id_to_pos.json') as fp:\n",
    "    id_to_pos = json.load(fp)\n",
    "with open('../../ics_cwe/pos_to_id.json') as fp:\n",
    "    pos_to_id = json.load(fp)\n",
    "graph_path = \"../../graph_network/data/\"\n",
    "with open(graph_path+'combined_nodes.json') as fp:\n",
    "    nodes_json = json.load(fp)\n",
    "# with open(graph_path+'combined_edges.json') as fp:\n",
    "#     edges_json = json.load(fp)\n",
    "# SemiSupervised\n",
    "with open(graph_path+'combined_edges.json') as fp:\n",
    "    edges_json = json.load(fp)\n",
    "\n",
    "with open(dir_name+'data/anchor_pos_neg_triple_{}.pkl'.format(sample), 'rb') as f:\n",
    "    anchor_pos_neg_triple=pickle.load(f)\n",
    "nn_dir = dir_name+'data/sample_{}/GCN_embeddings_graph/'.format(sample)\n",
    "if not os.path.exists(nn_dir):\n",
    "    os.makedirs(nn_dir)\n",
    "if(two_losses):\n",
    "    out_file = '/node_embeddings_gm_{}_tm_{}.npy'.format(margin1, margin2)\n",
    "else:\n",
    "    out_file = '/node_embeddings_gm_{}.npy'.format(margin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15ee59-a04f-4dda-a748-94dcf2d08b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_range = (0,203)\n",
    "weak_range = (203,1136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82252b86-da33-4b01-abfd-8d5f11d9c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_nodes = []\n",
    "positive_nodes = []\n",
    "negative_nodes = []\n",
    "for a,p,n in anchor_pos_neg_triple:\n",
    "    anchor_nodes.append(a)\n",
    "    positive_nodes.append(p)\n",
    "    negative_nodes.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d723f-89b4-44d5-b3da-8df5825df5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = list(range(0, weak_range[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08ba5d-2c80-4e73-890a-3937a7a7ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = [(int(e[0]), int(e[1])) for e in edges_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1447c1-97ec-4f8a-808d-8588b48e2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.from_edgelist(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9232bdc-7d94-446b-a43c-32bb0ee18938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph Features using node2vec\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "embeddings = np.array([model.wv[str(n)] for n in G.nodes()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d930baa-fe44-44ae-8612-b93926cb3bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(graph_path+'graph_features/node2vec.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456024c-ec52-45fe-a915-54166e2b0462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph Features using deepwalk\n",
    "# Train DeepWalk model\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from gensim.models import Word2Vec\n",
    "# Extract Structural Features\n",
    "degrees = np.array([G.degree(n) for n in G.nodes()])\n",
    "# Generate Random Walks for DeepWalk\n",
    "def generate_random_walks(G, num_walks, walk_length):\n",
    "    walks = []\n",
    "    nodes = list(G.nodes())\n",
    "    for _ in range(num_walks):\n",
    "        np.random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walk = [node]\n",
    "            while len(walk) < walk_length:\n",
    "                cur = walk[-1]\n",
    "                neighbors = list(G.neighbors(cur))\n",
    "                if len(neighbors) > 0:\n",
    "                    next_node = np.random.choice(neighbors)\n",
    "                    walk.append(next_node)\n",
    "                else:\n",
    "                    break\n",
    "            walks.append(walk)\n",
    "    return walks\n",
    "# Parameters for DeepWalk\n",
    "num_walks = 200\n",
    "walk_length = 30\n",
    "walks = generate_random_walks(G, num_walks, walk_length)\n",
    "\n",
    "# Train Word2Vec model on walks\n",
    "walks = [[str(node) for node in walk] for walk in walks]\n",
    "model = Word2Vec(sentences=walks, vector_size=64, window=5, min_count=0, sg=1, workers=4, epochs=10)\n",
    "\n",
    "# Extract embeddings\n",
    "node_ids = list(G.nodes())\n",
    "embeddings_array = np.array([model.wv[str(node)] for node in node_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456220ad-e0bf-4341-9bf4-cf4eff3ed68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array.shape\n",
    "np.save(graph_path+'graph_features/deepwalk.npy', embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fc6c0-06ba-4136-abcb-828c9f9b9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_emb = embeddings\n",
    "deepwalk_emb = embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb8628-69b0-4a76-b98e-88a437bde84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attack_range = (0,203)\n",
    "weak_range = (203,1136)\n",
    "# Generate embeddings for both sets of nodes\n",
    "attack_nodes = list(range(attack_range[0],attack_range[1]))\n",
    "weakness_nodes = list(range(weak_range[0],weak_range[1]))\n",
    "attack_embeddings = deepwalk_emb[attack_nodes]\n",
    "weakness_embeddings = deepwalk_emb[weakness_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c15498-eb70-4c39-9d5d-f454f671a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between all pairs of nodes\n",
    "weak_attack_matrix = cosine_similarity(weakness_embeddings, attack_embeddings)\n",
    "# Compute cosine similarity between all pairs of nodes\n",
    "attack_weak_matrix = cosine_similarity(attack_embeddings, weakness_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c57cc2-7de5-42ca-a9dd-4d18316e78be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "def histogram2(attack_weak_matrix):\n",
    "    # print(len(cosine_sim_pairs))\n",
    "    # print(len(cosine_sim_pairs[0]))\n",
    "    # for row in cosine_sim_pairs:\n",
    "    #     print(row[0][1], row[1][1])\n",
    "    #Extract the cosine similarity values from the filtered results\n",
    "    #cosine_sim_values = [pair[1] for row in cosine_sim_pairs for pair in row[-30:]]\n",
    "    cosine_sim_values=[]\n",
    "    for i in range(len(attack_weak_matrix)):\n",
    "        for j in range(len(attack_weak_matrix[0])):\n",
    "            cosine_sim_values.append(attack_weak_matrix[i][j])\n",
    "    #print(cosine_sim_values)\n",
    "    # Define the bins for the histogram\n",
    "    bins = np.arange(0, 1.1, 0.05)  # Bins from 0 to 1 with step size 0.1\n",
    "    \n",
    "    # Create the histogram\n",
    "    plt.hist(cosine_sim_values, bins=bins, edgecolor='black')\n",
    "    \n",
    "    # Set the x-axis and y-axis labels\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Set the title of the histogram\n",
    "    plt.title('Attack & Weak Positive')\n",
    "    # plt.savefig(dir2+'/histogram_pos_t_40.png',dpi=300)\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aaeb72-c494-4ebb-9614-b434e202c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram2(attack_weak_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bcd1f-f9e0-4711-8d1d-3b4d31a9fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchor_pos_neg(positive_threshold,negative_threshold):\n",
    "    anchor_pos_pair = []\n",
    "    anchor_neg_pair = []\n",
    "    # positive_threshold = 0.65  # Similarity threshold for positive pairs\n",
    "    # negative_threshold = 0.45  # Similarity threshold for negative pairs\n",
    "    for i, attack_node in enumerate(attack_nodes):\n",
    "        for j, weakness_node in enumerate(weakness_nodes):\n",
    "            if attack_weak_matrix[i, j] > positive_threshold:\n",
    "                anchor_pos_pair.append((attack_node,weakness_node, attack_weak_matrix[i, j]))\n",
    "            if attack_weak_matrix[i, j] < negative_threshold:\n",
    "                anchor_neg_pair.append((attack_node,weakness_node, attack_weak_matrix[i, j]))\n",
    "            \n",
    "    for i, weakness_node in enumerate(weakness_nodes):\n",
    "        for j, attack_node in enumerate(attack_nodes):\n",
    "            if weak_attack_matrix[i, j] > positive_threshold:\n",
    "                anchor_pos_pair.append((weakness_node,attack_node, weak_attack_matrix[i, j]))\n",
    "            if weak_attack_matrix[i, j] < negative_threshold:\n",
    "                anchor_neg_pair.append((weakness_node,attack_node, weak_attack_matrix[i, j]))\n",
    "    \n",
    "    anchor_pos_pair.sort(reverse=True, key=lambda x:x[2])\n",
    "    anchor_neg_pair.sort(key=lambda x:x[2])\n",
    "    print(len(anchor_pos_pair))\n",
    "    print(len(anchor_neg_pair))\n",
    "    anchor_pos_neg_triple = []\n",
    "    anchor_for_neg = [pair[0] for pair in anchor_neg_pair]\n",
    "    pos_pair = []\n",
    "    neg_pair = []\n",
    "    for anchor,pos,val in anchor_pos_pair:\n",
    "        if(anchor in anchor_for_neg):\n",
    "            idx = anchor_for_neg.index(anchor)\n",
    "            anchor_pos_neg_triple.append((anchor,pos,anchor_neg_pair[idx][1]))\n",
    "            pos_pair.append((pos,val))\n",
    "            neg_pair.append((anchor_neg_pair[idx][1],anchor_neg_pair[idx][2]))\n",
    "            anchor_for_neg.pop(idx)\n",
    "            anchor_neg_pair.pop(idx)\n",
    "    return anchor_pos_neg_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede4d0c-4af7-468a-a064-5789eab3e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_pos_neg_triple=get_anchor_pos_neg(positive_threshold=0.35,negative_threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5fa2f-df4b-4393-bd28-a4a6431956cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ass = set()\n",
    "ps=set()\n",
    "ns=set()\n",
    "for a,p,n in anchor_pos_neg_triple:\n",
    "    ass.add(a)\n",
    "    ps.add(p)\n",
    "    ns.add(n)\n",
    "print(len(ass))\n",
    "print(len(ps))\n",
    "print(len(ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf653e1-7f04-4f5e-b984-4b63951ee955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(graph_path+'graph_features/anchor_pos_neg_triple_4_node2vec.npy', 'wb') as f:\n",
    "    pickle.dump(anchor_pos_neg_triple, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
